{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "PATH_TO_DATA = '../../data/statefarm/'\n",
    "PATH_TO_IMGS = PATH_TO_DATA + 'imgs/'\n",
    "train_data_dir = PATH_TO_IMGS+'train/'\n",
    "validation_data_dir = PATH_TO_IMGS+'/validation/'\n",
    "test_data_dir = PATH_TO_IMGS+'/test/'\n",
    "img_width, img_height = 80,80 #150, 150\n",
    "\n",
    "labelMapping = { 'c0': 'normal driving', \n",
    "                 'c1': 'texting - right',\n",
    "                'c2': 'talking on the phone - right',\n",
    "                'c3': 'texting - left',\n",
    "                'c4': 'talking on the phone - left',\n",
    "                'c5': 'operating the radio',\n",
    "                'c6': 'drinking',\n",
    "                'c7': 'reaching behind',\n",
    "                'c8': 'hair and makeup',\n",
    "                'c9': 'talking to passenger' \n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18819 images belonging to 10 classes.\n",
      "Found 3034 images belonging to 10 classes.\n",
      "v5\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator #as ImageDataGeneratorBase\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        rotation_range=8,       #degrees#rotation_range: Int. Degree range for random rotations.\n",
    "        width_shift_range=0.05, #width_shift_range: Float (fraction of total width). Range for random horizontal shifts.\n",
    "        height_shift_range=0.05,  #height_shift_range: Float (fraction of total height). Range for random vertical shifts.\n",
    "        zoom_range=0.1,  \n",
    "        horizontal_flip=False)\n",
    "train_datagen.mean = np.load('mean_image'+str(img_width)+'.npy')\n",
    "train_datagen.std= np.load('std_image'+str(img_width)+'.npy')\n",
    "#this is the augmentation configuration we will use for testing: only rescaling + mean and std of training\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rescale=1./255)\n",
    "test_datagen.mean = np.load('mean_image'+str(img_width)+'.npy')\n",
    "test_datagen.std= np.load('std_image'+str(img_width)+'.npy')\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')  #label infered from folder\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical') #label infered from folder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def saveToFiles(model,filenameWithoutSuffix):\n",
    "    ''' will create X.json and X.h5 files'''\n",
    "    with open(filenameWithoutSuffix +'.json', \"w\") as json_file:\n",
    "        json_file.write(model.to_json() )\n",
    "    model.save_weights(filenameWithoutSuffix +'.h5')\n",
    "    print(\"Saved model to disk \"+ filenameWithoutSuffix)\n",
    "\n",
    "def loadFromFiles(filenameWithoutSuffix):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open(filenameWithoutSuffix+ '.json', 'r')\n",
    "    json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(json)\n",
    "    loaded_model.load_weights(filenameWithoutSuffix +'.h5')\n",
    "    return loaded_model\n",
    "print 'v5'\n",
    "#saveToFiles(loaded_model2,'model1_21epoc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18819 images belonging to 10 classes.\n",
      "will only work on 100 batches of them  \n",
      "running on # 1\n",
      "0.311151806362 0.378612648303 0.368988604246\n",
      "0.288531686315 0.332663278551 0.332838195427\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculat_std_and_mean_images_on_RGB_W_H():\n",
    "\n",
    "  \n",
    "    generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width,img_height),\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        class_mode=None)\n",
    "   \n",
    "\n",
    "    channel_to_mean_list = {'R':[],'G':[],'B':[]}\n",
    "    channel_to_std_list =  {'R':[],'G':[],'B':[]}\n",
    " \n",
    "    num_of_batches= 100#  10000/batch_size  #there are 20000 for both test and train so let's take subset  \n",
    "    print 'will only work on',num_of_batches,'batches of them  '\n",
    "    for i in range(0,num_of_batches):\n",
    "        if i%200==1:  \n",
    "            print 'running on #',i\n",
    "          \n",
    "        x= next(generator)[0]  #batch is always one!\n",
    "        for i,channel in enumerate (['R','G','B']):\n",
    "            channel_to_mean_list[channel].append(np.mean(x[i,:,:]))\n",
    "            channel_to_std_list[channel].append(np.std((x[i,:,:])))\n",
    "      \n",
    "    #create an array of the same shape\n",
    "    mean_image = np.empty([3,img_width,img_height])\n",
    "    std_image = np.empty([3,img_width,img_height])\n",
    "    \n",
    "    #calcute the mean of all the images together. mean will have 3 values reperated meanR,meanG,meanB\n",
    "  \n",
    "    for i,channel in enumerate ( ['R','G','B']):\n",
    "        mean_channel_list= channel_to_mean_list[channel]\n",
    "        mean_image[i] = np.mean(mean_channel_list) # mean of 100 values, is just mean\n",
    "        #print 'mean',mean_image[i][0,0],mean_channel_list\n",
    "        \n",
    "        std_channel_list= channel_to_std_list[channel]                                        \n",
    "        std_image[i] =  np.sqrt( np.sum(np.square(std_channel_list))/float(len(std_channel_list)))\n",
    "        #print 'std',std_image[i][0,0],std_channel_list\n",
    "    return mean_image,std_image\n",
    "   \n",
    "def calculateAndSaveOnce():  #must be called once at least!!!!\n",
    "    mean_image,std_image=calculat_std_and_mean_images_on_RGB_W_H() \n",
    "    print mean_image[0,0,0] ,mean_image[1,0,0] ,mean_image[2,0,0] # 0.305443632772 0.374382142377 0.364442742236\n",
    "    print std_image[0,0,0],std_image[1,0,0],std_image[2,0,0]      # 0.284977836867 0.330808235492 0.332097428032\n",
    "    np.save('mean_image'+str(img_height),mean_image)\n",
    "    np.save('std_image'+str(img_height),std_image)\n",
    "    \n",
    "calculateAndSaveOnce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "def if_prob_slim_change_it(all_probs):\n",
    "    ''' all_probs is of shape ROWSx10 '''\n",
    "    confidanceTracker ={'high':0, 'medium':0, 'low':0}\n",
    "    for i in range(0,all_probs.shape[0]):\n",
    "        confidance = np.sum(all_probs[i])  #usually much less than 1\n",
    "        if np.sum(all_probs[i])<0.3:\n",
    "            confidanceTracker['low']= 1+ confidanceTracker['low']\n",
    "            all_probs[i]+=0.1\n",
    "        elif np.sum(all_probs[i])<0.9:\n",
    "            confidanceTracker['medium']= 1+ confidanceTracker['medium']\n",
    "            all_probs[i]+=0.001\n",
    "        else:\n",
    "            confidanceTracker['high']= 1+ confidanceTracker['high']\n",
    "    print confidanceTracker\n",
    "\n",
    "\n",
    "def predict_all_directory(loaded_modelX,directory,limit_batches=0):\n",
    "    '''\n",
    "    limit_batches=0 means no limit(full directory). positive means we will cap after X batches'''\n",
    "    #let's make sure to recreate it, so generator will re-init\n",
    "    batch_size=32\n",
    "    submit_generator = test_datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode=None) #no labels    \n",
    "\n",
    "    num_of_batchs = len(submit_generator.filenames)/batch_size\n",
    "    mod = len(submit_generator.filenames)%batch_size\n",
    "    if mod: \n",
    "        num_of_batchs+=1\n",
    "    if (limit_batches>0):\n",
    "        num_of_batchs=limit_batches\n",
    "    \n",
    "    list_of_probs_batches=[]\n",
    "    for batch_i in range(0,num_of_batchs):\n",
    "        x_list= next(submit_generator)  #batch of 32 images\n",
    "        x = np.asarray(x_list)\n",
    "        curr_probs = loaded_modelX.predict( x, batch_size)\n",
    "        list_of_probs_batches.append(curr_probs)\n",
    "        if batch_i%50==1:\n",
    "            print batch_i, list_of_probs_batches[batch_i].shape\n",
    "    all_probs =  np.concatenate(list_of_probs_batches,axis=0)\n",
    "    return submit_generator.filenames[:all_probs.shape[0]] , all_probs\n",
    "\n",
    "def predict_to_csv(filenameNoPrefix,pathname_list,all_probs):\n",
    "    \n",
    "    # The expected out file should be:\n",
    "    #img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\n",
    "    #img_0.jpg,1,0,0,0,0,...,0\n",
    "    #img_1.jpg,0.3,0.1,0.6,0,...,0\n",
    "    name_list = [os.path.basename(x) for x in pathname_list]\n",
    "    data_dict = { 'img':name_list }\n",
    "    for i in range(0,10):\n",
    "        data_dict['c'+str(i)]= all_probs[:,i]\n",
    "\n",
    "    my_solution = pandas.DataFrame.from_dict(data_dict)\n",
    "\n",
    "    # Check that your data frame has 418 entries\n",
    "    print 'csv shape', my_solution.shape\n",
    "    print my_solution.describe()\n",
    "    pandas.set_option('display.width', 800)\n",
    "    print(my_solution)\n",
    "\n",
    "\n",
    "    # Write your solution to a csv file with the name my_solution.csv\n",
    "    filename='state_farm_PE_'+filenameNoPrefix+strftime(\"_%Y-%m-%d_%H-%M\", gmtime())+\".csv\"\n",
    "    my_solution.to_csv(filename,\n",
    "                       columns=['img','c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "                       , index=False,index_label = [\"img\"])\n",
    "    print 'save complete to file '+filename\n",
    "    \n",
    "    #now cut few rows\n",
    "    my_solution = my_solution[0:79726]\n",
    "    filename='state_farm_PE_'+filenameNoPrefix+'_cut79726_'+strftime(\"_%Y-%m-%d_%H-%M\", gmtime())+\".csv\"\n",
    "    my_solution.to_csv(filename,\n",
    "                       columns=['img','c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "                       , index=False,index_label = [\"img\"])\n",
    "    print 'save complete to file '+filename\n",
    " \n",
    "    for i in range(0,10):\n",
    "        my_solution['c'+str(i)] = my_solution['c'+str(i)].map('{:,.8f}'.format)\n",
    "        filename='state_farm_P8_'+filenameNoPrefix+strftime(\"_%Y-%m-%d_%H-%M\", gmtime())+\".csv\"\n",
    "    my_solution.to_csv(filename,\n",
    "                       columns=['img','c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "                       , index=False,index_label = [\"img\"])\n",
    "    print 'save complete to file '+filename\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "\n",
    "pred = [(0.5,0.5)]      \n",
    "act = [(1,0)]\n",
    "#print (logloss((1,0), (0.5,0.5))) #0.69\n",
    "#print (logloss((0,1), (1,0)))     #very wrong- 34.5\n",
    "#print (logloss((1,0), (1,0)))     # true - ~0 (9e-16)\n",
    "#print (logloss((1,0,0,0), (0,1,1,1))) # 0.1\n",
    "\n",
    "def score_batch(actual_class_list,pred_nparray,verbose=0):\n",
    "    '''\n",
    "        return score and misses indexes loss bigger than 0.1 on one item\n",
    "    '''\n",
    "    #create an array from one class\n",
    "    print 'calculating score of' , pred_nparray.shape[0], 'results'\n",
    "    if len(actual_class_list)!=pred_nparray.shape[0] :\n",
    "        raise ValueError('wrong shapes')\n",
    "    scores=[]\n",
    "    miss_scores=[]\n",
    "    miss_index=[]\n",
    "    for i,c in enumerate(actual_class_list):\n",
    "        act_list = [ 0  for _ in range(0,pred_nparray.shape[1])]\n",
    "        act_list[int(c)]=1\n",
    "        \n",
    "        pred_list = pred_nparray[i].tolist()\n",
    "        curr_score=logloss(act_list,pred_list )\n",
    "        scores.append(curr_score)\n",
    "        if curr_score>0.1:\n",
    "            miss_index.append(i)\n",
    "            miss_scores.append(curr_score)\n",
    "            if verbose:\n",
    "                print i,'WRONG',curr_score #,act_list,pred_list\n",
    "        elif verbose:\n",
    "                print i,'RIGHT\\t\\t\\t',curr_score #,act_list,pred_list\n",
    "       \n",
    "    if verbose:\n",
    "        print 'median'  ,np.median(np.array(scores))\n",
    "        print 'mean',np.mean(np.array(scores))\n",
    "        print 'misses >0.1 mean',np.mean(np.array(miss_scores))\n",
    "        print 'misses >0.1 median',np.median(np.array(miss_scores))\n",
    "    \n",
    "    return sum(scores)/pred_nparray.shape[0],miss_index\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_validation_score(model,limit_batches=0,verbose=0):\n",
    "    pathname_list,all_probs= predict_all_directory(model,validation_data_dir,limit_batches) #0 no limit\n",
    "    name_list = [os.path.dirname(x)[1] for x in pathname_list]\n",
    "    score,miss_indexes= score_batch(name_list,all_probs)\n",
    "    accuracy = 1- float(len(miss_indexes))/ len(name_list)\n",
    "    if verbose:\n",
    "        print '\\nSCORE=',score,'accuracy=',accuracy\n",
    "        print miss_indexes , 'of total',len(pathname_list)\n",
    "        print [pathname_list[i]  for i in miss_indexes]\n",
    "    return score,accuracy\n",
    "\n",
    "def create_csv(model,name):\n",
    "    pathname_list,all_probs= predict_all_directory(model,test_data_dir)\n",
    "    #if_prob_slim_change_it(all_probs)   \n",
    "    predict_to_csv(name,pathname_list,all_probs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"running statefarm-predict as main. doing nothing\"\n",
    "else:\n",
    "    print \"running statefarm-predict as import only. use me like this: %run 'statefarm-util.ipynb'   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def flat(history_list,key):\n",
    "        return sum([history.history[key] for history in history_list],[])\n",
    "    \n",
    "def graph_history(history_list):\n",
    "    #comibe histories\n",
    "    #keys= history_list[0].history.keys()\n",
    "\n",
    "    # red dash r--  'bs' Blue Square  'g^' Green Triangles\n",
    "    x_epocs= range(0,len(history_list))\n",
    "    \n",
    "    plt.plot(x_epocs, flat(history_list,'loss'),'b-',label='loss')\n",
    "    plt.plot(x_epocs, flat(history_list,'val_loss'),'r-',label='val_loss')\n",
    "    max_val = max( max(flat(history_list,'loss')), max(flat(history_list,'val_loss')))\n",
    "    plt.axis([0, len(x_epocs), 0, max_val ])\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    " \n",
    "    plt.plot(x_epocs, flat(history_list,'acc'),'b^--',label='acc')\n",
    "    plt.plot(x_epocs, flat(history_list,'val_acc'),'r^--',label='val_acc')\n",
    "    plt.axis([0, len(x_epocs), 0, 1])\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#combine it to the first one...\n",
    "def graph_history_3(history_list):\n",
    "   \n",
    "    \n",
    "    #comibe histories\n",
    "\n",
    "    x_epocs= range(0,len(history_list))\n",
    "\n",
    "    l1 = plt.plot(x_epocs, flat(history_list,'loss'),'b-',label='loss')\n",
    "    plt.plot(x_epocs, flat(history_list,'val_loss'),'r-',label='val_loss')\n",
    "             \n",
    "    l2 =plt.plot(        x_epocs, flat(history_list,'prob_loss'),'bo--',label='c2')\n",
    "    plt.plot(         x_epocs, flat(history_list,'val_prob_loss'),'ro--',label='c2')\n",
    "             \n",
    "    l3= plt.plot(         x_epocs, flat(history_list,'prob_aux0_loss'),'bs--',label='c0')\n",
    "    plt.plot(         x_epocs, flat(history_list,'val_prob_aux0_loss'),'rs--',label='c0')\n",
    "             \n",
    "    l4=plt.plot(         x_epocs, flat(history_list,'prob_aux1_loss'),'b^--',label='c1')\n",
    "    plt.plot(         x_epocs, flat(history_list,'val_prob_aux1_loss'),'r^--',label='c1')\n",
    "            \n",
    "    plt.axis([0, len(x_epocs), 0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()   \n",
    "    \n",
    "    plt.plot(\n",
    "             \n",
    "             x_epocs, flat(history_list,'prob_acc'),'bo--',\n",
    "             x_epocs, flat(history_list,'val_prob_acc'),'ro--',\n",
    "             \n",
    "             x_epocs, flat(history_list,'prob_aux0_acc'),'bs--',\n",
    "             x_epocs, flat(history_list,'val_prob_aux0_acc'),'rs--',\n",
    "             \n",
    "             x_epocs, flat(history_list,'prob_aux1_acc'),'b^--',\n",
    "             x_epocs, flat(history_list,'val_prob_aux1_acc'),'r^--',\n",
    "            )\n",
    "    \n",
    "    \n",
    "           \n",
    "    plt.axis([0, len(x_epocs), 0.5, 1.0])\n",
    "    plt.show()          \n",
    "    \n",
    "\n",
    "   \n",
    "    # red dash r--  'bs' Blue Square  'g^' Green Triangles\n",
    "   \n",
    "print 'done'       \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "def time_str(): \n",
    "    return strftime(\"_%Y-%m-%d_%H-%M\", gmtime())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
